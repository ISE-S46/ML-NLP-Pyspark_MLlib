{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Dates and Timestamp"
      ],
      "metadata": {
        "id": "SoQf_KBQNMbr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5yVxA0RkQm9W"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('date and time').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example CSV with date and timestamp columns\n",
        "data = [(\"2025-01-01\", \"2025-01-01 15:30:00\"),\n",
        "       (\"2025-01-02\", \"2025-01-02 10:00:00\")]\n",
        "columns = [\"date\", \"timestamp\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8_2vHQxXvrt",
        "outputId": "c1fad453-4a78-4cdf-8bf2-e9679bede7a2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------------+\n",
            "|      date|          timestamp|\n",
            "+----------+-------------------+\n",
            "|2025-01-01|2025-01-01 15:30:00|\n",
            "|2025-01-02|2025-01-02 10:00:00|\n",
            "+----------+-------------------+\n",
            "\n",
            "root\n",
            " |-- date: string (nullable = true)\n",
            " |-- timestamp: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert date to date and timestamp to timestamp"
      ],
      "metadata": {
        "id": "18Z21Pk1FAkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date, to_timestamp\n",
        "\n",
        "df = df.withColumn(\"date\", to_date(\"date\")) \\\n",
        "      .withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQNxvK_8mPBG",
        "outputId": "1855da29-1cd8-4322-b557-f5b4c1faab08"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- date: date (nullable = true)\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract Parts of Dates and Timestamps"
      ],
      "metadata": {
        "id": "43yETz1hFDFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import year, month, dayofmonth, hour, minute, second\n",
        "\n",
        "df.select(\n",
        "   year(\"date\").alias(\"Year\"),\n",
        "   month(\"date\").alias(\"Month\"),\n",
        "   dayofmonth(\"date\").alias(\"Day\"),\n",
        "   hour(\"timestamp\").alias(\"Hour\"),\n",
        "   minute(\"timestamp\").alias(\"Minute\"),\n",
        "   second(\"timestamp\").alias(\"Second\")\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dl62s7gXmVAF",
        "outputId": "d816ef65-2491-4ad2-a5ab-88ef2a878135"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+---+----+------+------+\n",
            "|Year|Month|Day|Hour|Minute|Second|\n",
            "+----+-----+---+----+------+------+\n",
            "|2025|    1|  1|  15|    30|     0|\n",
            "|2025|    1|  2|  10|     0|     0|\n",
            "+----+-----+---+----+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filtering and Comparing Dates"
      ],
      "metadata": {
        "id": "taetmKaSFG7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lit\n",
        "\n",
        "# Filter rows where date is after 2025-01-01\n",
        "df.filter(df[\"date\"] > lit(\"2025-01-01\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_7DZlOYmYOn",
        "outputId": "7d09d067-41c3-4755-a4cd-e8a9f9891c0f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------------+\n",
            "|      date|          timestamp|\n",
            "+----------+-------------------+\n",
            "|2025-01-02|2025-01-02 10:00:00|\n",
            "+----------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Formatting Dates and Timestamp"
      ],
      "metadata": {
        "id": "VzBru9reF0cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import date_format\n",
        "\n",
        "df.select(\n",
        "   date_format(\"date\", \"yyyy/MM/dd\").alias(\"FormattedDate\"),\n",
        "   date_format(\"timestamp\", \"HH:mm:ss\").alias(\"FormattedTime\")\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eefc1iXsmaMq",
        "outputId": "c1c25ff1-ee4c-49ae-f3b0-6765291b0034"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-------------+\n",
            "|FormattedDate|FormattedTime|\n",
            "+-------------+-------------+\n",
            "|   2025/01/01|     15:30:00|\n",
            "|   2025/01/02|     10:00:00|\n",
            "+-------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding or subtracting days"
      ],
      "metadata": {
        "id": "LY0N-ZVhF3S9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import date_add, date_sub\n",
        "\n",
        "df.select(\n",
        "   \"date\",\n",
        "   date_add(\"date\", 10).alias(\"DatePlus10Days\"),\n",
        "   date_sub(\"date\", 10).alias(\"DateMinus10Days\")\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oa_qk2DTmc4Q",
        "outputId": "6934e61b-d4cb-49ec-8f0e-959ae00fc289"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------+---------------+\n",
            "|      date|DatePlus10Days|DateMinus10Days|\n",
            "+----------+--------------+---------------+\n",
            "|2025-01-01|    2025-01-11|     2024-12-22|\n",
            "|2025-01-02|    2025-01-12|     2024-12-23|\n",
            "+----------+--------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling Time Intervals"
      ],
      "metadata": {
        "id": "fphUEyUJF_4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import current_date, datediff\n",
        "\n",
        "df.withColumn(\"DaysSince\", datediff(current_date(), \"date\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1xCosRfmf59",
        "outputId": "ebb13adc-9fe4-4e98-9f63-e14a66924335"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------------+---------+\n",
            "|      date|          timestamp|DaysSince|\n",
            "+----------+-------------------+---------+\n",
            "|2025-01-01|2025-01-01 15:30:00|      101|\n",
            "|2025-01-02|2025-01-02 10:00:00|      100|\n",
            "+----------+-------------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using SQL syntax for date"
      ],
      "metadata": {
        "id": "PzEBIqcEGDlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"dates\")\n",
        "spark.sql(\"SELECT date, YEAR(date) as Year FROM dates\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqcrYJnbmjVl",
        "outputId": "b400c9fe-a7f5-4e58-af73-dd4a1ab001f7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+\n",
            "|      date|Year|\n",
            "+----------+----+\n",
            "|2025-01-01|2025|\n",
            "|2025-01-02|2025|\n",
            "+----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handle Missing Date Data"
      ],
      "metadata": {
        "id": "aqQughpZGFh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a DataFrame with missing dates"
      ],
      "metadata": {
        "id": "4UrC2x_PGI4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample DataFrame with missing dates\n",
        "data = [(\"John\", None), (\"Sarah\", \"2025-01-01\"), (\"Mike\", None)]\n",
        "columns = [\"Name\", \"Date\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Show the original DataFrame\n",
        "print(\"Original DataFrame:\")\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSCQCx4KmoDe",
        "outputId": "e19b6074-7e4d-4aa7-8f0c-ff0a1761757e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+-----+----------+\n",
            "| Name|      Date|\n",
            "+-----+----------+\n",
            "| John|      NULL|\n",
            "|Sarah|2025-01-01|\n",
            "| Mike|      NULL|\n",
            "+-----+----------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Date: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert string to Date"
      ],
      "metadata": {
        "id": "u1IXku_lGMEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "# Convert the Date column to DateType\n",
        "df = df.withColumn(\"Date\", to_date(df[\"Date\"]))\n",
        "\n",
        "print(\"DataFrame After Casting Date Column:\")\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MGYX68KmpxH",
        "outputId": "d32af05a-c966-4985-ec34-1f244f5bb3e0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame After Casting Date Column:\n",
            "+-----+----------+\n",
            "| Name|      Date|\n",
            "+-----+----------+\n",
            "| John|      NULL|\n",
            "|Sarah|2025-01-01|\n",
            "| Mike|      NULL|\n",
            "+-----+----------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Date: date (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill NULL with today date"
      ],
      "metadata": {
        "id": "7d0NU_t9GPaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import current_date, when, to_date\n",
        "# Fill missing dates with today's date using `when`\n",
        "df_filled = df.withColumn(\n",
        "   \"Date\",\n",
        "   when(df[\"Date\"].isNull(), current_date()).otherwise(df[\"Date\"])\n",
        ")\n",
        "\n",
        "# Show the updated DataFrame\n",
        "df_filled.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pViDRPapmtjR",
        "outputId": "32a36025-2a41-4b9f-ddda-b2dfea6412a4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+\n",
            "| Name|      Date|\n",
            "+-----+----------+\n",
            "| John|2025-04-12|\n",
            "|Sarah|2025-01-01|\n",
            "| Mike|2025-04-12|\n",
            "+-----+----------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}